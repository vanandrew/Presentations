{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 class=\"text-center\">Generative Modeling of Neuroimaging Data using Generative\n",
    "Adversarial Networks</h1>\n",
    "<br>\n",
    "<h6 class=\"text-center\">Andrew Van</h6>\n",
    "<h6 class=\"text-center\">Advisor: Nico Dosenbach</h6>\n",
    "<script type=\"text/javascript\">\n",
    "$(window).load(function(){\n",
    "    Reveal.configure({\n",
    "        transition: 'fade', // none/fade/slide/convex/concave/zoom\n",
    "        //width: 1280,\n",
    "    })\n",
    "});    \n",
    "</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       "   $('div.input').hide();\n",
       "   $('div.prompt.output_prompt').css('opacity', 0);\n",
       " } else {\n",
       "   $('div.input').show();\n",
       "   $('div.prompt.output_prompt').css('opacity', 1);\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$(document).ready(code_toggle);\n",
       "</script>\n",
       "<a href=\"javascript:code_toggle()\"><button>Toggle Code</button></a>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    "   $('div.input').hide();\n",
    "   $('div.prompt.output_prompt').css('opacity', 0);\n",
    " } else {\n",
    "   $('div.input').show();\n",
    "   $('div.prompt.output_prompt').css('opacity', 1);\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$(document).ready(code_toggle);\n",
    "</script>\n",
    "<a href=\"javascript:code_toggle()\"><button>Toggle Code</button></a>\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">Overview</h3>\n",
    "\n",
    "- What are GANs?\n",
    "- Model Training and Results\n",
    "- Applications and Future Directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">What are Generative Adversarial Networks (GANs)?</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">What are GANs?</h3>\n",
    "\n",
    "- Take two networks and train them in an adversarial manner...\n",
    "\n",
    "<div>\n",
    "<img style=\"display:block;margin-right:auto;margin-left:auto;filter:invert(1);\" src=\"images/GANdiag.png\" width=50% height=auto>\n",
    "</div>\n",
    "<p style=\"text-align: center\">GAN Training Process</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">What are GANs?</h3>\n",
    "\n",
    "- ...and they can make really convincing fake images.\n",
    "\n",
    "<div>\n",
    "<img style=\"display:block;margin-right:auto;margin-left:auto;\" src=\"images/stylegan-teaser.png\" width=60% height=auto>\n",
    "</div>\n",
    "<p style=\"text-align: center\">These are not real people! <a href=\"https://arxiv.org/abs/1812.04948\" target=\"_blank\">StyleGAN</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">Fake Images? So What?</h3>\n",
    "\n",
    "- GANs capture an underlying process that generated our dataset.\n",
    "    - Creating realistic fake examples is the byproduct of a well-trained model.\n",
    "- A data-driven prior!\n",
    "        \n",
    "<div>\n",
    "<img style=\"display:block;margin-right:auto;margin-left:auto;filter:invert(1);\" src=\"images/density_estimation.png\" width=70% height=auto>\n",
    "</div>\n",
    "<p style=\"text-align: center\">Modelling a distribution given a sample</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">What is Generative Modeling?</h3>\n",
    "\n",
    "- Discriminative models\n",
    "    - Map high-dimensional data to a class label\n",
    "    - e.g. Given an image, is it a cat/dog?\n",
    "- Generative models answer a somewhat opposite question\n",
    "    - Given a sample drawn from a distribution, we want to learn an estimate of that distribution\n",
    "    - e.g. Given a set of cats/dogs, learn the images that generate each class.\n",
    "\n",
    "<div>\n",
    "<img style=\"display:block;margin-right:auto;margin-left:auto;\" src=\"images/highresimgsynth.gif\" width=40% height=auto>\n",
    "</div>\n",
    "<p style=\"text-align: center\">Generative Modeling: Estimating a distribution</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3><center>Taxonomy of generative modeling</center></h3>\n",
    "\n",
    "<div>\n",
    "    <img style=\"display:block;margin:auto;filter:invert(1);\" src=\"images/taxonomy.png\" width=60% height=auto>\n",
    "</div>\n",
    "<p style=\"text-align: center\">Types of generative models (maximum likelihood)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">GANs compared to other generative models</h3>\n",
    "\n",
    "- GANs are implicit density models\n",
    "    - A black box: we don't have access to the actual distribution, we can only draw samples from it.\n",
    "- Explicit models define a parameterization of the model.\n",
    "    - The difficulty comes in having an explicit model complex enough to capture the data complexity of dataset, while still being computationally tractable.\n",
    "        - Bound on complexity of model to ensure tractability\n",
    "    - e.g. Fully Visible Belief Networks (FVBNs), Variational Autoencoders\n",
    "- Advantages\n",
    "    - More computationally tractable than explicit models\n",
    "    - No approximations needed. Can learn any dataset distribution (given enough examples and layers).\n",
    "    - Subjectively, the research community has found that GANs provide better samples than any other method (at present)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">GAN Theory</h3>\n",
    "\n",
    "- Two networks: generator and discriminator setup in a two player zero-sum game\n",
    "    - Discriminator Objective: Pick out fake from real\n",
    "    - Generator Objective: Create a fake indistinguishable from real\n",
    "\n",
    "<div>\n",
    "<img style=\"display:block;margin-right:auto;margin-left:auto;filter:invert(1);\" src=\"images/GANdiag.png\" width=30% height=auto>\n",
    "</div>\n",
    "<p style=\"text-align: center\">GAN Training Process</p>\n",
    "\n",
    "$$ \\min_{G} \\max_{D} \\underset{\\mathbf{x} \\sim \\mathbb{P}_{r}}{\\mathbb{E}}[\\log D(\\mathbf{x})] + \\underset{\\mathbf{z} \\sim \\mathbb{P}_{\\mathbf{z}}}{\\mathbb{E}}[\\log (1 - D(G(\\mathbf{z})))] $$\n",
    "\n",
    "- From game theory, we find this system reaches Nash Equilibrium when $\\mathbb{P}_{r} = \\mathbb{P}_{g}$, and $D(x) = 0.5$ for all inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">Limitations</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">Non-Convergence</h3>\n",
    "\n",
    "- Optimization algorithms tend to at least converge to saddle/local optima\n",
    "- Game based algorithms may not approach an equilibrium\n",
    "\n",
    "<h3 class=\"text-center\">Low Support</h3>\n",
    "\n",
    "- When two distributions lie in a low dimensional manifold, they are (with very high probability) disjoint\n",
    "    - A perfect discriminator exists that can separate both exactly\n",
    "\n",
    "<div>\n",
    "<img style=\"display:block;margin-right:auto;margin-left:auto;background-color: #FFF;\" src=\"images/low_dim_manifold.png\" width=50% height=auto>\n",
    "</div>\n",
    "<p style=\"text-align: center\">Distributions of both networks lie in a lower dimensional space</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">Vanishing Gradients</h3>\n",
    "\n",
    "- Low support can lead to the discriminator improving too fast\n",
    "    - If the discriminator is perfect too early, generator doesn't have any gradients to backprop on\n",
    "- So slow discriminator learning?\n",
    "    - If the discriminator is bad, generator doesn't learn accurately\n",
    "\n",
    "<h3 class=\"text-center\">Mode Collapse</h3>\n",
    "\n",
    "- GAN collapse the value of several inputs to the same output\n",
    "\n",
    "<div>\n",
    "<img style=\"display:block;margin-right:auto;margin-left:auto;\" src=\"images/mode_collapse.png\" width=60% height=auto>\n",
    "</div>\n",
    "<p style=\"text-align: center\">Helvetica Scenario</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">Why does Mode Collapse happen?</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">Lack of Evaluation Metric</h3>\n",
    "\n",
    "- Unclear how to universally evaluate GANs quantitatively at present\n",
    "    - When to stop? When the images \"look\" good.\n",
    "- Popular measure in many GAN papers is Frechet Inception Distance or Inception Score\n",
    "    - Measures how \"natural\" looking an image is, using inception classifier\n",
    "    - Only works for natural image scenes, unsuitable for medical imaging applications\n",
    "    - Maybe substitute the inception network with a classifier trained on medical images?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">Model Training</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">Dataset Characterization</h3>\n",
    "\n",
    "- Human Connectome Project (HCP) 1200 Young Adult Dataset\n",
    "- 1113 T1s, 1783 Unique Volumes\n",
    "    - 1707 Unique Volumes after QC applied (issue code A)\n",
    "- 256 x 256 x 320, 0.7 mm voxel\n",
    "- Volumes unstacked in axial direction\n",
    "    - 256 slices of 256 x 320, zero-padded to 512 x 512\n",
    "- Total dataset size: 546,240 TI slices of 512 x 512 resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">Progressive GAN</h3>\n",
    "\n",
    "- Grows the architecture of the network during training\n",
    "    - Stablilizes training, since network has to learn less features first\n",
    "\n",
    "<div>\n",
    "<img style=\"display:block;margin-right:auto;margin-left:auto;\" src=\"images/progarch.png\" width=50% height=auto>\n",
    "</div>\n",
    "<p style=\"text-align: center\">Progressive GAN Architecture</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">Progressive GAN</h3>\n",
    "\n",
    "- Transition to next resolution occurs smoothly\n",
    "    - During transition, acts a residual block passing in lower resolution image to the output\n",
    "\n",
    "<div>\n",
    "<img style=\"display:block;margin-right:auto;margin-left:auto;\" src=\"images/proggrow.png\" width=50% height=auto>\n",
    "</div>\n",
    "<p style=\"text-align: center\">Growth Block</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">Wasserstein Distance Loss</h3>\n",
    "\n",
    "- Instead of the original GAN loss function, we use Wasserstein Distance\n",
    "    - Wasserstein distance attempts to solve the problem of low support and vanishing gradients\n",
    "        - Also solves mode collapse? \n",
    "    - Replaces the Discriminator with a Critic; Gives a measure of how different the fake samples are from the real (contrast with original discriminator which does a binary choice)\n",
    "- Wasserstein Distance Loss with Gradient Penalty:\n",
    "$$ \\min_{G} \\max_{D} \\underset{\\mathbf{z} \\sim \\mathbb{P}_{\\mathbf{z}}}{\\mathbb{E}}[D(G(\\mathbf{z}))] - \\underset{\\mathbf{x} \\sim \\mathbb{P}_{r}}{\\mathbb{E}}[D(\\mathbf{x})] + \\lambda \\underset{\\hat{\\mathbf{x}} \\sim \\mathbb{P}_{\\hat{\\mathbf{x}}}}{\\mathbb{E}}[(\\lVert \\nabla_{\\hat{\\mathbf{x}}} D(\\hat{\\mathbf{x}}) \\rVert_{2} - 1)^{2}] $$<div>\n",
    "<img style=\"display:block;margin-right:auto;margin-left:auto;\" src=\"images/WD.png\" width=30% height=auto>\n",
    "</div>\n",
    "<p style=\"text-align: center\">Wasserstein Distance</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">Wasserstein Distance with Gradient Penalty</h3>\n",
    "\n",
    "- Wasserstein Distance has 1-Lipshitz constraint (which was actually shown to be gradient norm = 1)\n",
    "    - Gradient of critic must be equal to 1 everywhere\n",
    "    - We add gradient penalty term to satisfy this constraint\n",
    "<br>\n",
    "$$ \\min_{G} \\max_{D} \\underset{\\mathbf{z} \\sim \\mathbb{P}_{\\mathbf{z}}}{\\mathbb{E}}[D(G(\\mathbf{z}))] - \\underset{\\mathbf{x} \\sim \\mathbb{P}_{r}}{\\mathbb{E}}[D(\\mathbf{x})] + \\lambda \\underset{\\hat{\\mathbf{x}} \\sim \\mathbb{P}_{\\hat{\\mathbf{x}}}}{\\mathbb{E}}[(\\lVert \\nabla_{\\hat{\\mathbf{x}}} D(\\hat{\\mathbf{x}}) \\rVert_{2} - 1)^{2}] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">Training Parameters</h3>\n",
    "\n",
    "- Trained with TensorFlow on NVIDIA Tesla V100 GPU on AWS/NVIDIA Geforce RTX 2080 Ti locally\n",
    "- Adam Optimizer with initial learning rate (step size) set to 1e-3 for each resolution layer\n",
    "- Resolution doubling occured every 1.2 million images (600k transition, 600k iterations @ resolution)\n",
    "- Mini-batch sizes: 4 × 4: 128, 8 × 8: 128, 16 × 16: 128, 32 × 32: 64, 64 × 64: 32, 128 × 128: 16, 256 × 256: 8, and 512 × 512: 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">Results</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">Training Process</h3>\n",
    "<br>\n",
    "<video style=\"display:block;margin-right:auto;margin-left:auto;\" width=60% height=auto controls loop>\n",
    "    <source src=\"videos/train.webm\" type=\"video/webm\">\n",
    "</video>\n",
    "<p style=\"text-align: center\">Snapshots throughout training (~4 days, 20 Hours)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">Loss Functions</h3>\n",
    "\n",
    "<div>\n",
    "<img style=\"display:block;margin-right:auto;margin-left:auto;\" src=\"images/loss.png\" width=35% height=auto>\n",
    "</div>\n",
    "<p style=\"text-align: center\">Loss Functions</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">Fake vs. Real</h3>\n",
    "<div>\n",
    "<img style=\"display:block;margin-right:auto;margin-left:auto;\" src=\"images/real.png\" width=90% height=auto>\n",
    "</div>\n",
    "<p style=\"text-align: center\">Real Images</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">Fake vs. Real</h3>\n",
    "<div>\n",
    "<img style=\"display:block;margin-right:auto;margin-left:auto;\" src=\"images/fake.png\" width=90% height=auto>\n",
    "</div>\n",
    "<p style=\"text-align: center\">Fake Images</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div>\n",
    "<img style=\"margin-right:0;margin-left:0;margin-top:15px;float:left;\" src=\"images/real0.png\" width=45% height=auto>\n",
    "<img style=\"margin-right:0;margin-left:0;margin-top:15px;float:right;\" src=\"images/fake0.png\" width=45% height=auto>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div>\n",
    "<img style=\"margin-right:0;margin-left:0;margin-top:15px;float:left;\" src=\"images/real1.png\" width=45% height=auto>\n",
    "<img style=\"margin-right:0;margin-left:0;margin-top:15px;float:right;\" src=\"images/fake1.png\" width=45% height=auto>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div>\n",
    "<img style=\"margin-right:0;margin-left:0;margin-top:15px;float:left;\" src=\"images/real2.png\" width=45% height=auto>\n",
    "<img style=\"margin-right:0;margin-left:0;margin-top:15px;float:right;\" src=\"images/fake2.png\" width=45% height=auto>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div>\n",
    "<img style=\"margin-right:0;margin-left:0;margin-top:15px;float:left\" src=\"images/real3.png\" width=45% height=auto>\n",
    "<img style=\"margin-right:0;margin-left:0;margin-top:15px;float:right\" src=\"images/fake3.png\" width=45% height=auto>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">Latent Vector Walk</h3>\n",
    "<br>\n",
    "<video style=\"display:block;margin-right:auto;margin-left:auto;\" width=40% height=auto controls loop>\n",
    "    <source src=\"videos/interp.webm\" type=\"video/webm\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">Latent Vector Walk</h3>\n",
    "<br>\n",
    "<div>\n",
    "<img style=\"display:block;margin-right:auto;margin-left:auto;background-color:#DDD\" src=\"images/latentwalk.svg\" width=100% height=auto>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">Nearest Neighbor</h3>\n",
    "\n",
    "- Output closest matching image in training set\n",
    "\n",
    "<div>\n",
    "<img style=\"margin-right:0;margin-left:10%;margin-top:15px;float:left\" src=\"images/fakeimg.png\" width=30% height=auto>\n",
    "<img style=\"margin-right:10%;margin-left:0;margin-top:15px;float:right\" src=\"images/realimg.png\" width=30% height=auto>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<p style=\"text-align: center\">(left) random output from generator (right) nearest neighbor to fake image in training set</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">Applications and Future Directions</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">Task-Based Assessments of Image Quality (TAIQ)</h3>\n",
    "\n",
    "- Evaluation of GANs based on their performance on a task\n",
    "    - This would be an evaluation of a system that a GAN is one component of\n",
    "    - e.g. Looking at reconstruction accuracy when using a GAN based reconstruction algorithm using various trained GAN models\n",
    "    - e.g. Doing an ROC analysis on a GAN based anomaly detector\n",
    "- TAIQ gives relevant metrics on the model with tasks that we care about\n",
    "    - Downside: Does not tell you how a GAN model fails, may still need other independent metrics for mode collapse, sample fidelity, etc.\n",
    "    - [Boraj 2018](https://arxiv.org/abs/1802.03446) gives comprehensive review of proposed evaluation methods in the literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">Future Directions</h3>\n",
    "\n",
    "- Explore better evaluation techniques for GAN training and model selection\n",
    "- Try newer, better GAN architectures such as [StyleGAN](https://arxiv.org/abs/1812.04948)\n",
    "- Investigate and define a reconstruction problem to apply GAN-based reconstruction techniques\n",
    "- Explore effectiveness of GAN anomaly detection frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">Underdetermined Reconstruction</h3>\n",
    "\n",
    "- In image reconstruction, we are interested in the following problem:\n",
    "\n",
    "$$y = Ax + \\eta$$\n",
    "\n",
    "where $x \\in \\mathbb{R}^{n}$ is the object to reconstruct, $y \\in \\mathbb{R}^{m}$ describes the measurements, $A \\in \\mathbb{R}^{m \\times n}$ is the system operator/measurement matrix, and $\\eta$ is additive noise. The goal is to recover the object, $x$, using measurements from our system, $y$.\n",
    "- In reconstruction applications like Super-Resolution or Compressed Sensing, the system is underdetermined.\n",
    "    - The $m < n$.\n",
    "- In order to guarantee unique recovery, further assumptions about the data must be used.\n",
    "    - One possible solution: sparsity (commonly used in compressed sensing)\n",
    "    - Unique recovery of an underdetermined system using sparsity assumptions through $\\ell_{1}$-regularization:\n",
    "   \n",
    "$$ \\min_{x} \\lVert x \\rVert_{1}\\\\ s.t.\\ Ax = y \\nonumber $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">Underdetermined Reconstruction with GANs</h3>\n",
    "\n",
    "- A stronger prior: reconstructions that lie in the range of a well-trained generator.\n",
    "    \n",
    "$$ Find\\ \\hat{\\mathbf{x}} = G(\\hat{\\mathbf{z}}) \\\\ s.t.\\ \\hat{\\mathbf{z}} = \\arg \\min_{\\mathbf{z}} \\lVert AG(\\mathbf{z}) - y \\rVert_{2}^{2}$$\n",
    "\n",
    "<div>\n",
    "<img style=\"display:block;margin-right:auto;margin-left:auto;filter:invert(1);\" src=\"images/lassogen.png\" width=60% height=auto>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">Open Questions for GAN based reconstruction</h3>\n",
    "\n",
    "- Reconstruction when solution is not in the range of the generator\n",
    "    - Early Stopping\n",
    "    - Sparse-Gen Framework\n",
    "- How does noise effect reconstruction performance?\n",
    "- Global convergence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">Anomaly Detection</h3>\n",
    "\n",
    "- Anomaly: a sample that is different from the rest of the dataset\n",
    "    - We are interested in anomaly detection for Quality Control\n",
    "- Deep learning methods like GANs can capture high dimensional datasets\n",
    "    - Potential improvement in anomaly detection performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">AnoGAN</h3>\n",
    "\n",
    "- Finding anomalies through residual and discrimination loss\n",
    "\n",
    "$$ L(\\mathbf{x}) = \\min_{\\mathbf{z}} \\alpha \\lVert \\mathbf{x} - G(\\mathbf{z}) \\rVert_{2}^{2} + \\beta \\lVert \\mathbf{f}(\\mathbf{x}) - \\mathbf{f}(G(\\mathbf{z})) \\rVert_{2}^{2}$$\n",
    "\n",
    "- Residual loss: How similar a new example from the most similar (L2) sample from the generator\n",
    "- Discrimination loss: How similar the new example is to the most similar sample from the generator in terms of discriminator features (feature matching)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">Open Questions for GAN-based Anomaly Detection</h3>\n",
    "\n",
    "- Are GANs an effective solution for our problem?\n",
    "    - Recent study showed k-nearest neighbor model outperformed GAN-based models for anomaloy detection (not enough anomalous samples for hyper-parameter selection).\n",
    "- What is the best way to incorporate GANs into an anomaly detection framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 class=\"text-center\">Acknowledgements</h3>\n",
    "\n",
    "- Nico Dosenbach, MD/PhD\n",
    "- Mark Anastasio, PhD\n",
    "- Sayantan Bhadra\n",
    "- Abhinav Jha, PhD"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
