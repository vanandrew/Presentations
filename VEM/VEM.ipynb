{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>\n",
    "    The Variational Approximation for Bayesian Inference\n",
    "</h1>\n",
    "<p>\n",
    "    Life after the EM Algorithm\n",
    "</p>\n",
    "<br>\n",
    "<p>\n",
    "    Dimitris G. Tzikas, Aristidis C. Likas, and Nikolaos P. Galatsanos\n",
    "</p>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<p style=\"font-size:10px;\">CMI Lab Journal Club, 11/22/2019, Andrew Van</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p>This subslide runs a bunch of scripts for this presentation.</p>\n",
    "<script>window.addEventListener('load', () => { Reveal.configure({ transition: 'fade' }); });</script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Motivation</h1>\n",
    "\n",
    "- Variational Methods?\n",
    "    - **How does Variational Expectation-Maximization work?**\n",
    "    - This applies to anything with the word \"Variational\" (e.g. Variational Autoencoder)\n",
    "    \n",
    "- Succinctly...\n",
    "    - Variational methods allow for the approximation of the posterior of latent variables\n",
    "    - Derives a lower bound for the marginal likelihood of the observed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Introduction</h1>\n",
    "\n",
    "- Expectation Maximization (EM) is awesome!\n",
    "    - Applications: Joint problems in image reconstruction, segmentation, registration, etc.\n",
    "    - We use it to find estimates of model parameters that rely on unobserved latent variables\n",
    "    - However, limited applicability when encountering complex models\n",
    "    \n",
    "- Variational approximation relaxes requirements of EM\n",
    "    - Can be applied to a wider range of models\n",
    "    - EM can be viewed as a special case of Variational Bayesian Inference\n",
    "        - a.k.a Varitational EM (VEM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Expectation-Maximization</h1>\n",
    "\n",
    "We want to find:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\underset{\\theta}{\\arg\\!\\max} \\; p(x; \\theta)\n",
    "$$\n",
    "\n",
    "where $x$ are observations, and $\\theta$ are parameters.\n",
    "\n",
    "In EM, we introduce unobserved latent variables (by marginalizing the latent variables):\n",
    "\n",
    "$$\n",
    "p(x;\\theta) = \\int p(x, z; \\theta) dz\n",
    "$$\n",
    "\n",
    "This expression is known as the **marginal likelihood**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h1>Why it's called Bayesian Inference</h1>\n",
    "\n",
    "Using the Marginal Likelihood and Bayes Rule, we can do inference on posterior of latent variables, $p(z | x; \\theta)$\n",
    "\n",
    "Marginal Likelihood:\n",
    "\n",
    "$$\n",
    "p(x; \\theta) = \\int p(x, z; \\theta) dz = \\int p(x | z; \\theta) p(z; \\theta) dz\n",
    "$$\n",
    "\n",
    "Bayes Rule:\n",
    "\n",
    "$$\n",
    "p(z | x; \\theta) = \\frac{p(x | z; \\theta) p(z; \\theta)}{p(x; \\theta)}\n",
    "$$\n",
    "\n",
    "- However, it's difficult to solve the integral in the Marginal Likelihood term\n",
    "    - Effort in developing techniques to get around or approximate integral\n",
    "        - Numerical Sampling (e.g Monte Carlo) or Deterministic Approximation(Variational Methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h1>Bayesian Inference vs. MAP Estimation</h1>\n",
    "\n",
    "- What is the difference between MAP estimation and Bayesian Inference?\n",
    "- Both are Bayesian because they place priors on parameters, $\\theta$ right?\n",
    "\n",
    "MAP Estimation:\n",
    "\n",
    "$$\n",
    "    \\hat{\\theta}_{MAP} = \\underset{\\theta}{\\arg\\!\\max} \\; p(x | \\theta) p(\\theta)\n",
    "$$\n",
    "\n",
    "Bayesian Inference:\n",
    "\n",
    "$$\n",
    "    p(\\theta | x) = \\frac{p(x | \\theta) p(\\theta)}{\\int p(x | \\theta) p(\\theta) d\\theta)}\n",
    "$$\n",
    "\n",
    "- They are distinctly different in objective!\n",
    "    - MAP generates a point estimate on $\\theta$ (the mode of the posterior) while Bayesian Inference calculates the full posterior distribution.\n",
    "    - MAP is also known as \"Poor Man's Bayesian Inference\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Evidence Lower Bound (ELBO)</h1>\n",
    "\n",
    "The marginal likelihood can be rewritten as:\n",
    "\n",
    "$$\n",
    "\\ln p(x; \\theta) = F(q, \\theta) + KL(q || p)\n",
    "$$\n",
    "\n",
    "with:\n",
    "\n",
    "$$\n",
    "F(q, \\theta) = \\int q(z) \\ln \\Big( \\frac{p(x, z; \\theta)}{q(z)} \\Big) dz\n",
    "$$\n",
    "\n",
    "- Since the KL divergence must be $\\geq$ 0, it follows that $\\ln p(x; \\theta) \\geq  F(q, \\theta)$\n",
    "- $F(q, \\theta)$ is a lower bound of the log-likelihood (known as the Evidence Lower Bound (ELBO))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h1>ELBO Derivation</h1>\n",
    "\n",
    "$$\n",
    "\\ln p(x; \\theta) = \\int q(z) \\ln p(x; \\theta) dz \\\\\n",
    "= \\int q(z) \\ln p(x; \\theta) dz \\\\\n",
    "= \\int q(z) \\ln \\Big( \\frac{p(x; \\theta) p(z | x; \\theta)}{p(z | x; \\theta)} \\Big) dz \\\\\n",
    "= \\int q(z) \\ln \\Big( \\frac{p(x, z; \\theta)}{p(z | x; \\theta)} \\Big) dz \\\\\n",
    "= \\int q(z) \\ln \\Big( \\frac{p(x, z; \\theta) q(z)}{p(z | x; \\theta) q(z)} \\Big) dz \\\\\n",
    "= \\int q(z) \\ln \\Big( \\frac{p(x, z; \\theta)}{q(z)} \\Big) dz - \\int q(z) \\ln \\Big( \\frac{p(z | x; \\theta)}{q(z)} \\Big) dz \\\\\n",
    "= F(q, \\theta) + KL(q || p)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Maximizing the ELBO</h1>\n",
    "\n",
    "We want to maximize the ELBO:\n",
    "\n",
    "$$\n",
    "F(q, \\theta) = \\int q(z) \\ln \\Big( \\frac{p(x, z; \\theta)}{q(z)} \\Big) dz\n",
    "$$\n",
    "\n",
    "- We use a two-step process to maximize the lower bound (given starting parameters $\\theta^{OLD}$)\n",
    "    - Step 1: Maximize $F(q, \\theta^{OLD})$ w/ respect to $q(z)$\n",
    "    - Step 2: Maximize $F(q, \\theta)$ w/ respect to $\\theta$ to generate $\\theta^{NEW}$\n",
    "\n",
    "- The lower bound is the same as maximizing the log likelihood when $KL(q || p) = 0$\n",
    "    - This implies that $q(z) = p(z | x; \\theta^{OLD})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Maximizing the ELBO (continued)</h1>\n",
    "\n",
    "Setting the latent posterior ($p(z | x; \\theta^{OLD})$) to be $q(z)$, we derive the familiar form for the EM algorithm:\n",
    "\n",
    "$$\n",
    "F(q, \\theta) = \\int p(z | x; \\theta^{OLD}) \\ln p(x, z; \\theta) dz - \\int p(z | x; \\theta^{OLD}) \\ln p(z | x; \\theta^{OLD}) dz \\\\\n",
    "= \\langle \\ln p(x, z; \\theta)\\rangle_{p(z | x; \\theta^{OLD})} + constant \\\\\n",
    "= Q(\\theta, \\theta^{OLD}) + constant\n",
    "$$\n",
    "\n",
    "- The EM algorithm\n",
    "    - E-step: Compute $Q(\\theta, \\theta^{OLD})$\n",
    "    - M-step: Find $\\theta$ that maximizes $Q(\\theta, \\theta^{OLD})$\n",
    "\n",
    "- The trick is that we must explicilty know $p(z | x; \\theta)$ to compute $Q(\\theta, \\theta^{OLD})$ (or $F(q, \\theta)$)\n",
    "    - Not always applicable in all problems, and we can't use EM :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Variational Approximation</h1>\n",
    "\n",
    "- We can bypass knowing $p(z | x; \\theta)$ exactly by assuming $q(z)$ has a specific form and optimize over $F(q, \\theta)$ using calculus of variations\n",
    "    - Thus the name \"Variational Approximation\"\n",
    "\n",
    "- Common form used: Mean Field approximation:\n",
    "\n",
    "$$\n",
    "q(z) = \\prod_{i=1}^{M} q_{i}(z_{i})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Variational Approximation (Continued)</h1>\n",
    "\n",
    "Applying the form $q(z) = \\prod_{i=1}^{M} q_{i}(z_{i})$ specified by the mean field approximation, we get the optimal $q(z)$ that maximizes the lower bound:\n",
    "\n",
    "$$\n",
    "q_{j}^{*}(z_j) = \\frac{\\exp(\\langle \\ln p(x, z; \\theta)\\rangle_{i \\neq j})}{\\int \\exp(\\langle \\ln p(x, z; \\theta)\\rangle_{i \\neq j}) dz_{j}} \n",
    "$$\n",
    "\n",
    "for each latent variable $j = 1, ..., M$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h1>Mean Field Solution Derivation</h1>\n",
    "\n",
    "$$\n",
    "F(q, \\theta) = \\int q(z) \\ln \\Big( \\frac{p(x, z; \\theta)}{q(z)} \\Big) dz \\\\\n",
    "= \\int \\prod_i q(z_i) \\ln p(x, z; \\theta) dz - \\sum_i \\int q(z_i) \\ln q(z_i) dz_i \\\\\n",
    "= \\int q(z_j) \\int \\Big( \\prod_{i \\neq j} q(z_i) \\ln p(x, z; \\theta) \\Big) \\prod_{i \\neq j} dz_i dz_j - \\int q(z_j) \\ln q(z_j) dz_j - \\sum_{i \\neq j} \\int q(z_i) \\ln q(z_i) dz_i \\\\\n",
    "= \\int q(z_j) \\ln \\Big( \\frac{\\exp(\\langle \\ln p(x, z; \\theta)\\rangle_{i \\neq j})}{q(z_j)} \\Big) dz_j - \\sum_{i \\neq j} \\int q(z_i) \\ln q(z_i) dz_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h1>Mean Field Solution Derivation (Continued)</h1>\n",
    "\n",
    "$$\n",
    "= \\int q(z_j) \\ln \\Big( \\frac{\\tilde{p}(x,z_{j}; \\theta)}{q(z_j)} \\Big) dz_j - \\sum_{i \\neq j} \\int q(z_i) \\ln q(z_i) dz_i \\\\\n",
    "= - KL(q_j || \\tilde{p}) - \\sum_{i \\neq j} \\int q(z_i) \\ln q(z_i) dz_i\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\ln \\tilde{p}(x,z_{j}; \\theta) = \\langle \\ln p(x, z; \\theta)\\rangle_{i \\neq j} = \\int \\ln p(x, z; \\theta) \\prod_{i \\neq j} (q_{i}dz_{i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h1>Mean Field Solution Derivation (Continued 2)</h1>\n",
    "\n",
    "Like before, $F(q, \\theta)$ is maximized when the KL divergence is 0, which occurs when $q_{j}(z_{j}) = \\tilde{p}(x,z_{j}; \\theta)$, so:\n",
    "\n",
    "$$\n",
    "\\ln q_{j}^{*}(z_{j}) = \\langle \\ln p(x, z; \\theta)\\rangle_{i \\neq j} + constant\n",
    "$$\n",
    "\n",
    "The additive constant can be obtained through normalization, so the final solution form is:\n",
    "\n",
    "$$\n",
    "q_{j}^{*}(z_j) = \\frac{\\exp(\\langle \\ln p(x, z; \\theta)\\rangle_{i \\neq j})}{\\int \\exp(\\langle \\ln p(x, z; \\theta)\\rangle_{i \\neq j}) dz_{j}} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Variational EM</h1>\n",
    "\n",
    "- With the mean field form of $q(z)$, the Variation EM (VEM) algorithm can be summarized as:\n",
    "    - E-step: Evaluate $q^{NEW}(z)$ to maximize $F(q,\\theta^{OLD})$\n",
    "    - M-step: Find $\\theta^{NEW}$ that maximizes $F(q^{NEW},\\theta)$\n",
    "    \n",
    "- A common case is where the model only contains latent variables and no parameters\n",
    "    - In this case, the VEM algorithm only has a Expectation step and no Maximization step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>A quick background on graphical models</h1>\n",
    "\n",
    "<div style=\"display:flex;flex-direction:row;margin-top:10px;\">\n",
    "    <div>\n",
    "        <img style=\"width:auto;\" src=\"images/graph_models.png\">\n",
    "    </div>\n",
    "    <div style=\"margin-left:10px;\">\n",
    "        <ul>\n",
    "            <li>Graphical models represent dependencies among random variables and parameters</li>\n",
    "            <ul>\n",
    "                <li>Double Circle: Observations</li>\n",
    "                <li>Circle: Latent Variables</li>\n",
    "                <li>Squares: Parameters</li>\n",
    "            </ul>\n",
    "            <li>Example (left):</li>\n",
    "            <ul>\n",
    "                <li>Each graph node $(s)$ has a set of parents $\\pi(s)$ they are conditioned on: $p(x_{s} | x_{\\pi(s)})$</li>\n",
    "                <li>Joint distribution can be defined by: $p(x; \\theta) = \\prod_{s} p(x_{s} | x_{\\pi(s)})$</li>\n",
    "                <li>So the joint distribution of the left graph can be defined as:</li>\n",
    "                $$p(a,b,c,d; \\theta) = p(a; \\theta_{1}) p(b | a; \\theta_{2}) p(c | a; \\theta_{3}) p(d | b,c; \\theta_{4})$$\n",
    "            </ul>\n",
    "            <li>In VEM, do the E-step on circles (Latent Variables) and M-step on squares (parameters)</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Example 1 (Linear Regression)</h1>\n",
    "\n",
    "- [GitHub](https://github.com/vanandrew/variational_em_demo/blob/master/regression-example.ipynb)\n",
    "\n",
    "<div style=\"text-align:center;display:flex;justify-content:center;\">\n",
    "    <img style=\"height:35vh;width:auto;\" src=\"images/linear_regression.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Example 2 (Gaussian Mixture Model)</h1>\n",
    "\n",
    "- [GitHub](https://github.com/vanandrew/variational_em_demo/blob/master/gmm-example.ipynb)\n",
    "\n",
    "<div style=\"display:flex;flex-direction:row;justify-content:center;\">\n",
    "    <div style=\"padding:10px;text-align:center\">\n",
    "        <div style=\"display:flex;justify-content:center;\">\n",
    "            <img style=\"height:15vh;width:auto;\" src=\"images/GMM.png\">\n",
    "        </div>\n",
    "        <p>GMM Model using EM</p>\n",
    "    </div>\n",
    "    <div style=\"padding:10px;text-align:center;\">\n",
    "        <div style=\"display:flex;justify-content:center;\">\n",
    "            <img style=\"height:15vh;width:auto;\" src=\"images/VGMM.png\">\n",
    "        </div>\n",
    "        <p>Full Bayesian GMM Model using VEM</p>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Useful Links and References</h1>\n",
    "\n",
    "This paper: https://ieeexplore.ieee.org/document/4644060\n",
    "\n",
    "Derivations for linear regression solutions: https://arxiv.org/abs/1301.3838\n",
    "\n",
    "Latex derivations: https://chrischoy.github.io/research/Expectation-Maximization-and-Variational-Inference/\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
